{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f0308e4",
   "metadata": {},
   "source": [
    "## 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f89357ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_test.txt')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fb86fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>document</td>\n",
       "      <td>label</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>6222902</td>\n",
       "      <td>인간이 문제지.. 소는 뭔죄인가..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>8549745</td>\n",
       "      <td>평점이 너무 낮아서...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>9311800</td>\n",
       "      <td>이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>2376369</td>\n",
       "      <td>청춘 영화의 최고봉.방황과 우울했던 날들의 자화상</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150000</th>\n",
       "      <td>9619869</td>\n",
       "      <td>한국 영화 최초로 수간하는 내용이 담긴 영화</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150001 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0                                  1      2\n",
       "0             id                           document  label\n",
       "1        9976970                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "2        3819312  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "3       10265843                  너무재밓었다그래서보는것을추천한다      0\n",
       "4        9045019      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "...          ...                                ...    ...\n",
       "149996   6222902                인간이 문제지.. 소는 뭔죄인가..      0\n",
       "149997   8549745                      평점이 너무 낮아서...      1\n",
       "149998   9311800    이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?      0\n",
       "149999   2376369        청춘 영화의 최고봉.방황과 우울했던 날들의 자화상      1\n",
       "150000   9619869           한국 영화 최초로 수간하는 내용이 담긴 영화      0\n",
       "\n",
       "[150001 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('~/aiffel/sentiment_classification/data/ratings_train.txt',sep=\"\t\",header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b76b3a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150001 entries, 0 to 150000\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   0       150001 non-null  object\n",
      " 1   1       149996 non-null  object\n",
      " 2   2       150001 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ba2bb8",
   "metadata": {},
   "source": [
    "training data의 document에서 결측치 관찰됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7503307a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50001 entries, 0 to 50000\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       50001 non-null  object\n",
      " 1   1       49998 non-null  object\n",
      " 2   2       50001 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "f = pd.read_csv('~/aiffel/sentiment_classification/data/ratings_test.txt',sep=\"\t\",header=None)\n",
    "f.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574b9a5f",
   "metadata": {},
   "source": [
    "test data의 document에서 결측치 관찰됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "868d16b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25858</th>\n",
       "      <td>2172111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55738</th>\n",
       "      <td>6369843</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110015</th>\n",
       "      <td>1034280</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126783</th>\n",
       "      <td>5942978</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140722</th>\n",
       "      <td>1034283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0    1  2\n",
       "25858   2172111  NaN  1\n",
       "55738   6369843  NaN  1\n",
       "110015  1034280  NaN  0\n",
       "126783  5942978  NaN  0\n",
       "140722  1034283  NaN  0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = df[df[[1]].isnull().any(axis=1)].head(15)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bb8a3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25857, 55737, 110014, 126782, 140721]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_train = [index-1 for index in k.index] #데이터 프레임 적용된 상태에서 인덱스가 1 늘어났기 때문에 원본 데이터와 맞쳐줌\n",
    "a_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f649692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5747</th>\n",
       "      <td>402110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7900</th>\n",
       "      <td>5026896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27098</th>\n",
       "      <td>511097</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0    1  2\n",
       "5747    402110  NaN  1\n",
       "7900   5026896  NaN  0\n",
       "27098   511097  NaN  1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_te = f[f[[1]].isnull().any(axis=1)].head(15)\n",
    "k_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e6998b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5746, 7899, 27097]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_test = [index-1 for index in k_te.index] #데이터 프레임 적용된 상태에서 인덱스가 1 늘어났기 때문에 원본 데이터와 맞쳐줌\n",
    "a_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e49556",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리\n",
    "불용어, 결측치, 토큰화, 중복제거 등 시행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f49afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58536d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef load_data(train_data, test_data, num_words=10000):\\n    train_data.drop_duplicates(subset=[\\'document\\'], inplace=True)\\n    train_data = train_data.dropna(how = \\'any\\') \\n    test_data.drop_duplicates(subset=[\\'document\\'], inplace=True)\\n    test_data = test_data.dropna(how = \\'any\\') \\n    \\n    def nrmz(dataset): \\n\\n        dataset.drop_duplicates(subset = [\\'document\\'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\\n        dataset[\\'document\\'] = dataset[\\'document\\'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\\n        dataset[\\'document\\'] = dataset[\\'document\\'].str.replace(\\'^ +\\', \"\") # 공백은 empty 값으로 변경\\n        dataset[\\'document\\'].replace(\\'\\', np.nan, inplace=True) # 공백은 Null 값으로 변경\\n        dataset = dataset.dropna(how=\\'any\\') # Null 값 제거\\n        \\n    nrmz(train_data)\\n    nrmz(test_data)\\n    \\n    #형태소 분석기를 통한 불용어 처리\\n    okt = Okt()\\n    X_train = []\\n    for sentence in tqdm(train_data[\\'document\\']):\\n        tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\\n        stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\\n        X_train.append(stopwords_removed_sentence)\\n        \\n    X_test = []\\n    for sentence in tqdm(test_data[\\'document\\']):\\n        tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\\n        stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\\n        X_test.append(stopwords_removed_sentence)\\n        \\n    words = np.concatenate(X_train).tolist()\\n    counter = Counter(words)\\n    counter = counter.most_common(10000-4)\\n    vocab = [\\'\\', \\'\\', \\'\\', \\'\\'] + [key for key, _ in counter]\\n    word_to_index = {word:index for index, word in enumerate(vocab)}\\n        \\n    def wordlist_to_indexlist(wordlist):\\n        return [word_to_index[word] if word in word_to_index else word_to_index[\\'\\'] for word in wordlist]\\n        \\n    X_train = list(map(wordlist_to_indexlist, X_train))\\n    X_test = list(map(wordlist_to_indexlist, X_test))\\n        \\n    return X_train, np.array(list(train_data[\\'label\\'])), X_test, np.array(list(test_data[\\'label\\'])), word_to_index\\n    \\nX_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\"\"\"\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "    \n",
    "    def nrmz(dataset): \n",
    "\n",
    "        dataset.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "        dataset['document'] = dataset['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
    "        dataset['document'] = dataset['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n",
    "        dataset['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "        dataset = dataset.dropna(how='any') # Null 값 제거\n",
    "        \n",
    "    nrmz(train_data)\n",
    "    nrmz(test_data)\n",
    "    \n",
    "    #형태소 분석기를 통한 불용어 처리\n",
    "    okt = Okt()\n",
    "    X_train = []\n",
    "    for sentence in tqdm(train_data['document']):\n",
    "        tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "        stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "        X_train.append(stopwords_removed_sentence)\n",
    "        \n",
    "    X_test = []\n",
    "    for sentence in tqdm(test_data['document']):\n",
    "        tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "        stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "        X_test.append(stopwords_removed_sentence)\n",
    "        \n",
    "    words = np.concatenate(X_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['', '', '', ''] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "        \n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index[''] for word in wordlist]\n",
    "        \n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "        \n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
    "    \n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2671b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "    \n",
    "    X_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_train.append(temp_X)\n",
    "\n",
    "    X_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_test.append(temp_X)\n",
    "    \n",
    "    words = np.concatenate(X_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['', '', '', ''] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "        \n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index[''] for word in wordlist]\n",
    "        \n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "        \n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
    "    \n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39e2d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "385b4f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c444309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  15.96940191154864\n",
      "문장길이 최대 :  116\n",
      "문장길이 표준편차 :  12.843571191092\n",
      "pad_sequences maxlen :  35\n",
      "전체 문장의 0.9108268190171958%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(X_train) + list(X_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 1.5 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca5253fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist([len(review) for review in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4d8018",
   "metadata": {},
   "source": [
    "우선 문장 최대 길이를 (평균 + 1.5 * std)로 지정\n",
    "그래프 상으로 20이 넘어가는 시점부터 샘플 수가 급격히 줄어드는 것을 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2a36911",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n",
    "\n",
    "# 처음 몇 개 인덱스는 사전에 정의되어 있습니다\n",
    "word_to_index[\"<PAD>\"] = 0\n",
    "word_to_index[\"<BOS>\"] = 1\n",
    "word_to_index[\"<UNK>\"] = 2  # unknown\n",
    "word_to_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3ae4e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146182, 35)\n"
     ]
    }
   ],
   "source": [
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre', # 혹은 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8b731e",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa149ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 35)          700000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 256)         62976     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 16)          28688     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 791,809\n",
      "Trainable params: 791,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "vocab_size = 20000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 35  # 워드 벡터의 차원 수 (변경 가능한 하이퍼파라미터)\n",
    "\n",
    "# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.Conv1D(256, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "212ed556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146182, 35)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c8a3d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set 20000건 분리\n",
    "x_val = X_train[:20000]   \n",
    "y_val = y_train[:20000]\n",
    "\n",
    "# validation set을 제외한 나머지 15000건\n",
    "partial_x_train = X_train[20000:]  \n",
    "partial_y_train = y_train[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bed954c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbcdb891",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_928/3267091976.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_to_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2VecKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 4"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "word2vec_file_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/word2vec.txt'\n",
    "f = open(word2vec_file_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))  # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀을 씁니다.\n",
    "\n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록합니다. \n",
    "vectors = model.get_weights()[0]\n",
    "\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "vector = word_vectors['computer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545ddb12",
   "metadata": {},
   "source": [
    "## 학습 시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47796723",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
